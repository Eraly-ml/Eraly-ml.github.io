---
title: "KazBERT: A Custom BERT Model for the Kazakh Language"
collection: publications
permalink: /publication/2025-kazbert
excerpt: 'A BERT-based model fine-tuned for Kazakh language tasks using Masked Language Modeling on a Kazakh, Russian, English text corpus.'
date: 2025-01-01
venue: 'Preprint'
paperurl: '/files/KazBERT.pdf'
citation: 'Gainulla, Y. (2025). "KazBERT: A Custom BERT Model for the Kazakh Language."'
---

KazBERT is a BERT-based model specifically designed and fine-tuned for Kazakh language tasks. The model is trained using Masked Language Modeling (MLM) on a multilingual text corpus containing Kazakh, Russian, and English texts.

## Key Features
- Custom tokenizer optimized for Kazakh language
- Trained on diverse Kazakh text corpus
- Supports downstream NLP tasks for Kazakh

[Download paper here](/files/KazBERT.pdf)

[View model on Hugging Face](https://huggingface.co/Eraly-ml/KazBERT)
